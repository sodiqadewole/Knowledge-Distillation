{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFOlc1v9pOpD"
      },
      "source": [
        "#### Knowledge Distillation\n",
        "\n",
        "Knowledge is a training technique where small models are trained based on knowledge transfer from larger and computationally more expensive models without losing validity. This allows deployment on smaller and less powerful hardware leading to faster inference and more efficient evaluation.\n",
        "\n",
        "In this notebook we cover experiments focused on comparing accuracy of lightweight neural network with a more powerful network using distillation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYwag_qKo_xx",
        "outputId": "c137176a-3dee-4b45-c62d-fa32e1bc3614"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else 'cpu'\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3smnFGYNy4ss"
      },
      "source": [
        "Load dataset (CIFAR-10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCGN2kYsy3Sx",
        "outputId": "50e5a1d3-bedf-4e27-bcdb-e762fafa0898"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 41.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing input Normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.456, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# load cifar-10\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loaders = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loaders = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFsVwJDvzr-I"
      },
      "source": [
        "Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Nvu8jrXly3Pb"
      },
      "outputs": [],
      "source": [
        "# Teacher Model\n",
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(TeacherModel, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(32 * 8 * 8, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        # x = x.view(x.size(0), -1)\n",
        "        # x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class StudentModel(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(StudentModel, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(16 * 8 * 8, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256, num_classes),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBb1qK191uqz"
      },
      "source": [
        "Create training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LgZ7AYGy3M1",
        "outputId": "6474285e-a589-4647-974b-223005e938c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 1.3964465220870874\n",
            "Epoch 2/10, Loss: 0.9504235522521426\n",
            "Epoch 3/10, Loss: 0.792588456207529\n",
            "Epoch 4/10, Loss: 0.6896231275842623\n",
            "Epoch 5/10, Loss: 0.6144788938638804\n",
            "Epoch 6/10, Loss: 0.5494618303787983\n",
            "Epoch 7/10, Loss: 0.49458054317842665\n",
            "Epoch 8/10, Loss: 0.4420514222606064\n",
            "Epoch 9/10, Loss: 0.40226563325394754\n",
            "Epoch 10/10, Loss: 0.36865541019746106\n",
            "Test Accuracy: 76.730000%\n"
          ]
        }
      ],
      "source": [
        "def train(model, train_loader, epochs, learning_rate, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(inputs)\n",
        "            # calculate loss\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            logits = model(inputs)\n",
        "            _, predicted = torch.max(logits.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "torch.manual_seed(42)\n",
        "# Instantiate, train and evaluate the teacher model\n",
        "teacher = TeacherModel().to(device)\n",
        "train(teacher, train_loaders, 10, 0.001, device)\n",
        "test_accuracy_teacher = test(teacher, test_loaders, device)\n",
        "\n",
        "# Instantiate the student model\n",
        "torch.manual_seed(42)\n",
        "student_1 = StudentModel().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GzNL6tMgy3Ka"
      },
      "outputs": [],
      "source": [
        "# instantiate one more student model to compare performance\n",
        "torch.manual_seed(42)\n",
        "student_2 = StudentModel().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8fIPQh-y3H-",
        "outputId": "f23cb3a3-7e25-4ec2-adc6-38b2f0064de7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norm of 1st layer of student-1 model: tensor(2.3274, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "Norm of 1st layer of student-2 model: tensor(2.3274, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# To ensure we have two different student models\n",
        "print(\"Norm of 1st layer of student-1 model:\", torch.norm(student_1.features[0].weight))\n",
        "print(\"Norm of 1st layer of student-2 model:\", torch.norm(student_2.features[0].weight))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbXfV8A2y3Fw",
        "outputId": "421debcc-d314-47e7-9b08-f1dea1409381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters in teacher model: 1,186,986\n",
            "Total parameters in student model: 267,738\n"
          ]
        }
      ],
      "source": [
        "# Compare parameters of the teacher and student models\n",
        "total_params_teacher = sum(p.numel() for p in teacher.parameters())\n",
        "print(f\"Total parameters in teacher model: {total_params_teacher:,}\")\n",
        "\n",
        "total_params_student = sum(p.numel() for p in student_1.parameters())\n",
        "print(f\"Total parameters in student model: {total_params_student:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL8QsYMTy3C8",
        "outputId": "86ccc3b2-5fdd-4f8b-9faa-ddfef1e07cc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 1.5056005944986173\n",
            "Epoch 2/10, Loss: 1.2171901727423948\n",
            "Epoch 3/10, Loss: 1.1019190862355634\n",
            "Epoch 4/10, Loss: 1.0210292486431043\n",
            "Epoch 5/10, Loss: 0.966056467474574\n",
            "Epoch 6/10, Loss: 0.9174672881965442\n",
            "Epoch 7/10, Loss: 0.8839037964868424\n",
            "Epoch 8/10, Loss: 0.8416863677218137\n",
            "Epoch 9/10, Loss: 0.8139965129859003\n",
            "Epoch 10/10, Loss: 0.7895370634163127\n",
            "Test Accuracy: 70.150000%\n"
          ]
        }
      ],
      "source": [
        "# Train the student network\n",
        "train(student_1, train_loaders, 10, 0.001, device)\n",
        "test_accuracy_student_1 = test(student_1, test_loaders, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7854Asr4vZv",
        "outputId": "8226c59f-6bd6-4bd8-84a4-8aaa23e44691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Teacher accuracy: 76.73%\n",
            "Student accuracy: 70.15%\n"
          ]
        }
      ],
      "source": [
        "# Compare teacher accuracy with student accuracy\n",
        "print(f\"Teacher accuracy: {test_accuracy_teacher:.2f}%\")\n",
        "print(f\"Student accuracy: {test_accuracy_student_1:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKoEcpad5Gcu"
      },
      "source": [
        "#### Knowledge Distillation\n",
        "\n",
        "Knowledge distillation incorporates additional loss into the traditional crossentropy loss which based on the softmax output of the teacher network. We assume that the output activate of the teacher network carries additional information that can be leverage by the student network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cir8x0_-4vUn",
        "outputId": "25a4b1a2-dbfe-4206-f610-53766b087392"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 1.4863279846013355\n",
            "Epoch 2/10, Loss: 1.3549058321491836\n",
            "Epoch 3/10, Loss: 1.2576321841353346\n",
            "Epoch 4/10, Loss: 1.1868097400268935\n",
            "Epoch 5/10, Loss: 1.1198706733601174\n",
            "Epoch 6/10, Loss: 1.0753767711427205\n",
            "Epoch 7/10, Loss: 1.0390705127088005\n",
            "Epoch 8/10, Loss: 1.0074640433196826\n",
            "Epoch 9/10, Loss: 0.9701840968235679\n",
            "Epoch 10/10, Loss: 0.9427998823582974\n",
            "Test Accuracy: 70.580000%\n",
            "Teacher accuracy: 76.73%\n",
            "Student accuracy without teacher: 70.15%\n",
            "Student accuracy with CE + KD: 70.58%\n"
          ]
        }
      ],
      "source": [
        "def train_knowledge_distillation(teacher_model, student_model, train_loader, epochs, learning_rate, device, alpha=0.5, temperature=3, soft_target_loss_weight=0.5, ce_loss_weight=0.5):\n",
        "    teacher_model.eval()\n",
        "    student_model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher_model(inputs)\n",
        "\n",
        "            student_logits = student_model(inputs)\n",
        "\n",
        "            # apply softmax to teacher logits\n",
        "            teacher_softmax = nn.functional.softmax(teacher_logits / temperature, dim=1)\n",
        "            # apply log on the softmax output from the student logits\n",
        "            student_prob = nn.functional.log_softmax(student_logits / temperature, dim=1)\n",
        "\n",
        "            # calculate the soft target loss\n",
        "            soft_targets_loss = torch.sum(teacher_softmax * (teacher_softmax.log() - student_prob)) / student_prob.size()[0] * (temperature ** 2)\n",
        "\n",
        "            # true label loss\n",
        "            true_labels_loss = criterion(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of two losses\n",
        "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * true_labels_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Train with Knowledge distillation\n",
        "train_knowledge_distillation(teacher, student_2, train_loaders, epochs=10, learning_rate=0.001, temperature=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
        "test_student_accuracy_ce_and_kd = test(student_2, test_loaders, device)\n",
        "\n",
        "# Compare the student test accuracy with and without the teacher, after distillation\n",
        "print(f\"Teacher accuracy: {test_accuracy_teacher:.2f}%\")\n",
        "print(f\"Student accuracy without teacher: {test_accuracy_student_1:.2f}%\")\n",
        "print(f\"Student accuracy with CE + KD: {test_student_accuracy_ce_and_kd:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
